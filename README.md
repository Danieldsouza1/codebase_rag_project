# ğŸ§  Codebase RAG Assistant

A Retrieval-Augmented Generation (RAG) system that dynamically ingests GitHub repositories, performs semantic code retrieval across large multi-language codebases, and generates grounded answers and automated code review insights using LLMs.

This project demonstrates how modern AI-powered developer tools (like Copilot Chat or Sourcegraph Cody) are built at a system level.

---

## ğŸš€ Features

- ğŸ”— **Dynamic GitHub Repository Ingestion**
  - Clone and analyze any public GitHub repository
- ğŸ§  **Semantic Code Search**
  - FAISS-based vector search over code and documentation
- â“ **Code Q&A Mode**
  - Ask natural-language questions about the codebase
- ğŸ” **Automated Code Review Mode**
  - Detect potential issues, code smells, and improvement suggestions
- ğŸŒ **Multi-Language Support**
  - Python, JavaScript, TypeScript, Java, Go, Markdown (configurable)
- ğŸ–¥ï¸ **Interactive Streamlit UI**
  - Simple web interface for real-time analysis
- ğŸ§± **Modular, Production-Style Architecture**

---

## ğŸ—ï¸ Architecture Overview

GitHub Repo
â†“
Code Loader (filtered file ingestion)
â†“
Text Chunking
â†“
Embeddings (Sentence Transformers)
â†“
FAISS Vector Store
â†“
Retriever
â†“
LLM (Groq - LLaMA 3.1)
â†“
Answer / Code Review Output


---

## ğŸ“‚ Project Structure

 codebase-rag-assistant/
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ loaders/
â”‚ â”‚ â”œâ”€â”€ code_loader.py
â”‚ â”‚ â””â”€â”€ github_loader.py
â”‚ â”œâ”€â”€ chunking/
â”‚ â”‚ â””â”€â”€ splitter.py
â”‚ â”œâ”€â”€ embeddings/
â”‚ â”‚ â””â”€â”€ embedder.py
â”‚ â”œâ”€â”€ vectorstore/
â”‚ â”‚ â””â”€â”€ faiss_store.py
â”‚ â”œâ”€â”€ llm/
â”‚ â”‚ â”œâ”€â”€ generator.py
â”‚ â”‚ â””â”€â”€ analyzer.py
â”‚ â””â”€â”€ app.py
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ github_repo/
â”‚
â”œâ”€â”€ streamlit_app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md


---

## âš™ï¸ How It Works

1. The user provides a GitHub repository URL
2. The repository is cloned locally
3. Source and documentation files are loaded and chunked
4. Chunks are embedded using a sentence-transformer model
5. FAISS performs similarity-based retrieval
6. Retrieved context is passed to an LLM
7. The LLM generates grounded answers or review insights

---

## â–¶ï¸ Running the Project

### 1ï¸âƒ£ Create & activate virtual environment
```bash
python -m venv venv
venv\Scripts\activate

pip install -r requirements.txt
python -m src.app
streamlit run streamlit_app.py

## ğŸ§  Why RAG?
Large Language Models alone hallucinate when analyzing large codebases.
RAG ensures responses are grounded in retrieved source code, improving accuracy, transparency, and reliability.

### ğŸ”® Future Improvements

File-level citations in answers

Embedding cache for faster re-runs

Chat history & memory

Deployment to Streamlit Cloud / Hugging Face Spaces

Advanced reranking for improved retrieval quality